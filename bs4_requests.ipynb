{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c0664a-aad0-458b-b6e5-665525ab0ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in ./venv/lib/python3.11/site-packages (5.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec11ae4-e76e-4a86-b6be-4ef03cfb0490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.31.0\n",
      "4.12.3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "\n",
    "print(requests.__version__)\n",
    "print(bs4.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e9399d-13b3-461a-990b-f4233489e35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "URL= \"https://www.naver.com/\"\n",
    "req = requests.get(URL)\n",
    "print(req.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d4d344-41c7-4bb0-ab91-0715e5171bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li>애플</li>, <li>삼성</li>, <li>노키아</li>, <li>LG</li>]\n",
      "['애플', '삼성', '노키아', 'LG']\n",
      "   회사명\n",
      "0   애플\n",
      "1   삼성\n",
      "2  노키아\n",
      "3   LG\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd \n",
    "\n",
    "with open('index.html', 'r', encoding='UTF8') as f:\n",
    "    # step 01 : 데이터 수집\n",
    "    contents = f.read()\n",
    "\n",
    "    #step 02 : 데이터 파싱(순수한 HTML 파일을 BeautifulSoup 객체로 변환)\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "    # print(soup)\n",
    "    \n",
    "#     print(soup.h2)\n",
    "#     print(soup.ul)\n",
    "#     print(\"-----\")\n",
    "#    print(soup.ul.li)\n",
    "    # 4개의 li 태그에 있는 회사명을 모두 가져오는 것이 목적\n",
    "\n",
    "    # step 03 : 데이터 수집 위한 특정 태그 찾기\n",
    "    companies = []\n",
    "    print(soup.find_all('li'))\n",
    "\n",
    "    # step 04 : 데이터 가공\n",
    "    for tag in soup.find_all('li'):\n",
    "        companies.append(tag.text)\n",
    "    print(companies)\n",
    "    \n",
    "    # step 05 : 처리된 데이터 저장 pandas 데이터 저장\n",
    "    crawling_dict = {'회사명': companies}\n",
    "    result = pd.DataFrame(crawling_dict)\n",
    "    print(result)\n",
    "\n",
    "    #step 06 : csv or 데이터베이스(DB)로 내보내기\n",
    "    result.to_csv(\"result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "649ed569-705b-4725-ac33-28eea3ebede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                노래제목\n",
      "0      Love wins all\n",
      "1               Wife\n",
      "2              To. X\n",
      "3           Love 119\n",
      "4      Perfect Night\n",
      "..               ...\n",
      "95             밤, 바다\n",
      "96       Baggy Jeans\n",
      "97        I Love You\n",
      "98  그대가 있는 곳, 언제 어디든\n",
      "99           사랑을 하다가\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "\n",
    "def crawling(soup) :\n",
    "    # print(soup)\n",
    "    tbody = soup.find(\"tbody\")\n",
    "    result = []\n",
    "    for p in tbody.find_all('p', class_ = 'title'):\n",
    "        result.append(p.get_text().strip())\n",
    "    return result\n",
    "\n",
    "def main() :\n",
    "    custom_header = {\n",
    "        'referer' : 'https://music.bugs.co.kr/',\n",
    "        'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = \"https://music.bugs.co.kr/chart\" # 크롤링 하려는 웹사이트\n",
    "    req = requests.get(url, headers = custom_header)\n",
    "    \n",
    "    soup = BeautifulSoup(req.text, \"html.parser\") # HTML 파일을 BeautifulSoup 객체로 변환 시키겠다.\n",
    "    crawling(soup)\n",
    "\n",
    "    titles = crawling(soup)\n",
    "    print(pd.DataFrame({\"노래제목\" : titles}))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b926bef-a0b0-4bfe-961e-dd1f4e550c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           노래제목\n",
      "0                                 Love wins all\n",
      "1                                         To. X\n",
      "2                                        비의 랩소디\n",
      "3                                 Perfect Night\n",
      "4                                      Love 119\n",
      "..                                          ...\n",
      "95                 건물 사이에 피어난 장미 (Rose Blossom)\n",
      "96                                         GODS\n",
      "97                                     여섯 번째 여름\n",
      "98  Yes or No (Feat. 허윤진 of LE SSERAFIM, Crush)\n",
      "99                               I Love My Body\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "\n",
    "def crowling(soup):\n",
    "    tbody=soup.find('tbody')\n",
    "    result=[]\n",
    "    for song_name in tbody.find_all('div', class_='ellipsis rank01'): # 여기 in 뒤에 tbody로 바꿈\n",
    "        result.append(song_name.get_text().strip())\n",
    "    return result # 리턴값 추가해줌\n",
    "\n",
    "def main():\n",
    "    custom_header={\n",
    "        'urls' : 'https://www.melon.com/',\n",
    "        'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = 'https://www.melon.com/chart/index.htm'\n",
    "    req = requests.get(url, headers=custom_header)\n",
    "    \n",
    "    soup=BeautifulSoup(req.text,\"html.parser\")\n",
    "    # print(crowling(soup))\n",
    "\n",
    "    titles=crowling(soup)\n",
    "    songchart=pd.DataFrame({'노래제목': titles})\n",
    "\n",
    "    print(songchart)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450477fe-c94e-4f6a-a618-a2a02f3db21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "\n",
    "def crowling(soup):\n",
    "    table= soup.find('table', class_='type2')\n",
    "    tbody=table.soup.find('tbody')\n",
    "    result_keys=[]\n",
    "    for k in tbody.find('tr'): # 여기서 우리는 객체의 방식으로 들고와야 하기 때문에 해당하는 key값과 value값을 따로 들고와서 딕셔너리 형태로 데이터를 줘야함\n",
    "        result_keys.append(k) # 예상 결과값은 날짜, 종가, 전일비... str값으로 들어갈 것이다.\n",
    "    return result_keys\n",
    "    for v in tbody.find_all('span', class_='tah')\n",
    "    \n",
    "company_code = '005930' # 삼성전자\n",
    "url =\"https://finance.naver.com/item/sise_day.nhn?code=\" + company_code\n",
    "    \n",
    "headers = { \n",
    "             'referer' : 'https://finance.naver.com/item/sise.naver?code=005930',\n",
    "             'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "            }\n",
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b84cb111-16b4-4c35-9933-560ede673aa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xb3 in position 194: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://finance.naver.com/item/sise_day.naver?code=005930&page=1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m company_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m005930\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# 삼성전자\u001b[39;00m\n\u001b[1;32m     10\u001b[0m url \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://finance.naver.com/item/sise_day.nhn?code=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m company_code\n",
      "File \u001b[0;32m~/Desktop/streamlit-mulcamp/venv/lib/python3.11/site-packages/pandas/io/html.py:1246\u001b[0m, in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1231\u001b[0m     [\n\u001b[1;32m   1232\u001b[0m         is_file_like(io),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1236\u001b[0m     ]\n\u001b[1;32m   1237\u001b[0m ):\n\u001b[1;32m   1238\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal html to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1243\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1244\u001b[0m     )\n\u001b[0;32m-> 1246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/streamlit-mulcamp/venv/lib/python3.11/site-packages/pandas/io/html.py:1009\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m retained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retained\n\u001b[1;32m   1011\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n",
      "File \u001b[0;32m~/Desktop/streamlit-mulcamp/venv/lib/python3.11/site-packages/pandas/io/html.py:989\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    978\u001b[0m p \u001b[38;5;241m=\u001b[39m parser(\n\u001b[1;32m    979\u001b[0m     io,\n\u001b[1;32m    980\u001b[0m     compiled_match,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    985\u001b[0m     storage_options,\n\u001b[1;32m    986\u001b[0m )\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 989\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseekable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io\u001b[38;5;241m.\u001b[39mseekable():\n",
      "File \u001b[0;32m~/Desktop/streamlit-mulcamp/venv/lib/python3.11/site-packages/pandas/io/html.py:249\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_tables(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs)\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "File \u001b[0;32m~/Desktop/streamlit-mulcamp/venv/lib/python3.11/site-packages/pandas/io/html.py:659\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._build_doc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_doc\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m--> 659\u001b[0m     bdoc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_build_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bdoc, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         udoc \u001b[38;5;241m=\u001b[39m bdoc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding)\n",
      "File \u001b[0;32m~/Desktop/streamlit-mulcamp/venv/lib/python3.11/site-packages/pandas/io/html.py:651\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._setup_build_doc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_build_doc\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 651\u001b[0m     raw_text \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_text:\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo text parsed from document: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/streamlit-mulcamp/venv/lib/python3.11/site-packages/pandas/io/html.py:146\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(obj, encoding, storage_options)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    139\u001b[0m     is_url(obj)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file_exists(obj))\n\u001b[1;32m    142\u001b[0m ):\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    144\u001b[0m         obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mencoding, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m    145\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m--> 146\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[1;32m    148\u001b[0m     text \u001b[38;5;241m=\u001b[39m obj\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xb3 in position 194: invalid start byte"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "\n",
    "url='https://finance.naver.com/item/sise_day.naver?code=005930&page=1'\n",
    "dfs=pd.read_html(url)\n",
    "\n",
    "\n",
    "\n",
    "company_code = '005930' # 삼성전자\n",
    "url =\"https://finance.naver.com/item/sise_day.nhn?code=\" + company_code\n",
    "    \n",
    "headers = { \n",
    "             'referer' : 'https://finance.naver.com/item/sise.naver?code=005930',\n",
    "             'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "            }\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(req.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb6788e8-7f8c-4fe5-94a5-7602c97fc79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            날짜       종가     전일비       시가       고가       저가         거래량\n",
      "0          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "1   2024.01.31  72700.0  1600.0  73400.0  74000.0  72500.0  13080752.0\n",
      "2   2024.01.30  74300.0   100.0  75000.0  75300.0  73700.0  12244418.0\n",
      "3   2024.01.29  74400.0  1000.0  73800.0  75200.0  73500.0  13976521.0\n",
      "4   2024.01.26  73400.0   700.0  73700.0  74500.0  73300.0  11160062.0\n",
      "5   2024.01.25  74100.0   100.0  74200.0  74800.0  73700.0  11737747.0\n",
      "6          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "7          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "8          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "9   2024.01.24  74000.0  1200.0  75200.0  75200.0  73500.0  12860661.0\n",
      "10  2024.01.23  75200.0   100.0  75700.0  75800.0  74300.0  14786224.0\n",
      "11  2024.01.22  75100.0   400.0  75900.0  76000.0  75000.0  19673375.0\n",
      "12  2024.01.19  74700.0  3000.0  73500.0  74700.0  73000.0  23363427.0\n",
      "13  2024.01.18  71700.0   700.0  71600.0  72000.0  70700.0  17853397.0\n",
      "14         NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "            날짜       종가     전일비       시가       고가       저가         거래량\n",
      "0          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "1   2024.01.17  71000.0  1600.0  73100.0  73300.0  71000.0  22683660.0\n",
      "2   2024.01.16  72600.0  1300.0  73500.0  73700.0  72500.0  14760415.0\n",
      "3   2024.01.15  73900.0   800.0  73200.0  74000.0  73200.0  13212339.0\n",
      "4   2024.01.12  73100.0   100.0  73000.0  74100.0  72800.0  13038939.0\n",
      "5   2024.01.11  73200.0   400.0  72900.0  73600.0  72700.0  57691266.0\n",
      "6          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "7          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "8          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "9   2024.01.10  73600.0  1100.0  75000.0  75200.0  73200.0  20259529.0\n",
      "10  2024.01.09  74700.0  1800.0  77400.0  77700.0  74300.0  26019249.0\n",
      "11  2024.01.08  76500.0   100.0  77000.0  77500.0  76400.0  11088724.0\n",
      "12  2024.01.05  76600.0     0.0  76700.0  77100.0  76400.0  11304316.0\n",
      "13  2024.01.04  76600.0   400.0  76100.0  77300.0  76100.0  15324439.0\n",
      "14         NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "            날짜       종가     전일비       시가       고가       저가         거래량\n",
      "0          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "1   2024.01.03  77000.0  2600.0  78500.0  78800.0  77000.0  21753644.0\n",
      "2   2024.01.02  79600.0  1100.0  78200.0  79800.0  78200.0  17142847.0\n",
      "3   2023.12.28  78500.0   500.0  77700.0  78500.0  77500.0  17797536.0\n",
      "4   2023.12.27  78000.0  1400.0  76700.0  78000.0  76500.0  20651042.0\n",
      "5   2023.12.26  76600.0   700.0  76100.0  76700.0  75700.0  13164909.0\n",
      "6          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "7          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "8          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "9   2023.12.22  75900.0   900.0  75800.0  76300.0  75400.0  14515608.0\n",
      "10  2023.12.21  75000.0   200.0  74600.0  75000.0  74300.0  13478766.0\n",
      "11  2023.12.20  74800.0  1400.0  74200.0  74900.0  73800.0  16870156.0\n",
      "12  2023.12.19  73400.0   500.0  73000.0  73400.0  72800.0   8907632.0\n",
      "13  2023.12.18  72900.0   400.0  73300.0  73400.0  72800.0   9690551.0\n",
      "14         NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "            날짜       종가     전일비       시가       고가       저가         거래량\n",
      "0          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "1   2023.12.15  73300.0   200.0  73800.0  74000.0  73200.0  15419815.0\n",
      "2   2023.12.14  73100.0   300.0  74100.0  74300.0  72500.0  27567593.0\n",
      "3   2023.12.13  72800.0   700.0  73300.0  73500.0  72800.0  13116766.0\n",
      "4   2023.12.12  73500.0   500.0  73300.0  73500.0  73100.0  13758646.0\n",
      "5   2023.12.11  73000.0   400.0  72800.0  73000.0  72200.0   9861960.0\n",
      "6          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "7          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "8          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "9   2023.12.08  72600.0  1100.0  72100.0  72800.0  71900.0  10859463.0\n",
      "10  2023.12.07  71500.0   200.0  71800.0  71900.0  71100.0   8862017.0\n",
      "11  2023.12.06  71700.0   500.0  71800.0  72100.0  71600.0   8123087.0\n",
      "12  2023.12.05  71200.0  1400.0  72300.0  72400.0  71200.0  12129682.0\n",
      "13  2023.12.04  72600.0   600.0  72800.0  72900.0  72400.0  10229267.0\n",
      "14         NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "            날짜       종가     전일비       시가       고가       저가         거래량\n",
      "0          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "1   2023.12.01  72000.0   800.0  72400.0  72500.0  71700.0   9871284.0\n",
      "2   2023.11.30  72800.0   100.0  72700.0  72800.0  72200.0  15783714.0\n",
      "3   2023.11.29  72700.0     0.0  72400.0  72800.0  72200.0   9283933.0\n",
      "4   2023.11.28  72700.0  1400.0  71400.0  72700.0  71300.0  13283081.0\n",
      "5   2023.11.27  71300.0   400.0  71500.0  72100.0  71100.0   9113857.0\n",
      "6          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "7          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "8          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "9   2023.11.24  71700.0   700.0  72400.0  72600.0  71700.0   6676685.0\n",
      "10  2023.11.23  72400.0   400.0  73000.0  73200.0  72200.0   6775614.0\n",
      "11  2023.11.22  72800.0     0.0  72200.0  73000.0  71900.0  11105143.0\n",
      "12  2023.11.21  72800.0   100.0  73100.0  73400.0  72700.0   9712881.0\n",
      "13  2023.11.20  72700.0   200.0  72100.0  73000.0  72100.0  10610157.0\n",
      "14         NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "            날짜       종가     전일비       시가       고가       저가         거래량\n",
      "0          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "1   2023.11.17  72500.0   300.0  72300.0  73000.0  72300.0  11494644.0\n",
      "2   2023.11.16  72800.0   600.0  72500.0  73000.0  72300.0  15860451.0\n",
      "3   2023.11.15  72200.0  1400.0  71600.0  72200.0  71500.0  20148677.0\n",
      "4   2023.11.14  70800.0   400.0  71000.0  71100.0  70600.0   9567984.0\n",
      "5   2023.11.13  70400.0   100.0  71300.0  71300.0  70300.0   9246919.0\n",
      "6          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "7          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "8          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "9   2023.11.10  70500.0   200.0  70000.0  70500.0  69500.0   9684347.0\n",
      "10  2023.11.09  70300.0   400.0  69900.0  70800.0  69600.0  12301373.0\n",
      "11  2023.11.08  69900.0  1000.0  71300.0  71400.0  69700.0  12901310.0\n",
      "12  2023.11.07  70900.0     0.0  70600.0  70900.0  70000.0  17228731.0\n",
      "13  2023.11.06  70900.0  1300.0  69800.0  70900.0  69300.0  22228489.0\n",
      "14         NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "            날짜       종가     전일비       시가       고가       저가         거래량\n",
      "0          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "1   2023.11.03  69600.0   100.0  69700.0  70200.0  69500.0  10322234.0\n",
      "2   2023.11.02  69700.0  1100.0  70000.0  70000.0  69400.0  16350031.0\n",
      "3   2023.11.01  68600.0  1700.0  67500.0  68900.0  67300.0  13775256.0\n",
      "4   2023.10.31  66900.0   400.0  67600.0  68300.0  66900.0  14488892.0\n",
      "5   2023.10.30  67300.0     0.0  66800.0  67800.0  66700.0  10139270.0\n",
      "6          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "7          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "8          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "9   2023.10.27  67300.0   600.0  67100.0  67300.0  66700.0  11334726.0\n",
      "10  2023.10.26  66700.0  1300.0  67000.0  67900.0  66700.0  15517624.0\n",
      "11  2023.10.25  68000.0   500.0  68800.0  68800.0  67900.0  10610703.0\n",
      "12  2023.10.24  68500.0   100.0  68700.0  68800.0  67700.0  12791710.0\n",
      "13  2023.10.23  68400.0   400.0  68700.0  69100.0  68200.0  11625959.0\n",
      "14         NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "            날짜       종가     전일비       시가       고가       저가         거래량\n",
      "0          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "1   2023.10.20  68800.0   700.0  68900.0  69200.0  68100.0  15204495.0\n",
      "2   2023.10.19  69500.0  1000.0  69700.0  70000.0  69400.0  13985012.0\n",
      "3   2023.10.18  70500.0  1100.0  68900.0  70500.0  68800.0  16493184.0\n",
      "4   2023.10.17  69400.0  2100.0  67700.0  69900.0  67400.0  17299253.0\n",
      "5   2023.10.16  67300.0   700.0  67900.0  68500.0  66800.0  12599299.0\n",
      "6          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "7          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "8          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "9   2023.10.13  68000.0   900.0  68000.0  68500.0  67700.0   9724086.0\n",
      "10  2023.10.12  68900.0   700.0  68600.0  69700.0  68200.0  19311380.0\n",
      "11  2023.10.11  68200.0  1800.0  68600.0  69400.0  67900.0  25209349.0\n",
      "12  2023.10.10  66400.0   400.0  66200.0  67600.0  66200.0  19889202.0\n",
      "13  2023.10.06  66000.0   700.0  67100.0  67300.0  66000.0  14386527.0\n",
      "14         NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "            날짜       종가     전일비       시가       고가       저가         거래량\n",
      "0          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "1   2023.10.05  66700.0   800.0  67300.0  67400.0  66700.0  16108313.0\n",
      "2   2023.10.04  67500.0   900.0  67400.0  67700.0  66700.0  23361149.0\n",
      "3   2023.09.27  68400.0   200.0  68600.0  69100.0  68200.0  14886491.0\n",
      "4   2023.09.26  68600.0   800.0  70000.0  70000.0  68400.0  13143470.0\n",
      "5   2023.09.25  69400.0   600.0  68500.0  69700.0  68500.0  13582516.0\n",
      "6          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "7          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "8          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "9   2023.09.22  68800.0   100.0  68300.0  68900.0  68300.0   9897840.0\n",
      "10  2023.09.21  68900.0   700.0  69200.0  69800.0  68800.0  10796336.0\n",
      "11  2023.09.20  69600.0   200.0  70000.0  70300.0  69500.0  10873015.0\n",
      "12  2023.09.19  69800.0   400.0  70400.0  70800.0  69600.0  11820188.0\n",
      "13  2023.09.18  70200.0  1800.0  71300.0  71700.0  70200.0  16040727.0\n",
      "14         NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "            날짜       종가    전일비       시가       고가       저가         거래량\n",
      "0          NaN      NaN    NaN      NaN      NaN      NaN         NaN\n",
      "1   2023.09.15  72000.0  300.0  71700.0  72300.0  71400.0  17823512.0\n",
      "2   2023.09.14  71700.0  800.0  71200.0  71800.0  70800.0  21041407.0\n",
      "3   2023.09.13  70900.0  400.0  71100.0  71600.0  70300.0  15955797.0\n",
      "4   2023.09.12  70500.0  300.0  70800.0  71000.0  70400.0  11688599.0\n",
      "5   2023.09.11  70800.0  500.0  70400.0  70800.0  70000.0  11785462.0\n",
      "6          NaN      NaN    NaN      NaN      NaN      NaN         NaN\n",
      "7          NaN      NaN    NaN      NaN      NaN      NaN         NaN\n",
      "8          NaN      NaN    NaN      NaN      NaN      NaN         NaN\n",
      "9   2023.09.08  70300.0  100.0  70200.0  70300.0  69600.0  10688118.0\n",
      "10  2023.09.07  70400.0  400.0  70000.0  70600.0  69600.0  13741241.0\n",
      "11  2023.09.06  70000.0  700.0  70700.0  70800.0  69700.0  11414620.0\n",
      "12  2023.09.05  70700.0  500.0  70900.0  71500.0  70200.0  12330239.0\n",
      "13  2023.09.04  71200.0  200.0  72900.0  72900.0  70700.0  26286495.0\n",
      "14         NaN      NaN    NaN      NaN      NaN      NaN         NaN\n",
      "            날짜       종가     전일비       시가       고가       저가         거래량\n",
      "0          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "1   2023.09.01  71000.0  4100.0  66800.0  71000.0  66700.0  29738235.0\n",
      "2   2023.08.31  66900.0   200.0  67100.0  67200.0  66400.0  15964630.0\n",
      "3   2023.08.30  67100.0   300.0  67300.0  67700.0  67100.0   9181223.0\n",
      "4   2023.08.29  66800.0     0.0  66900.0  67200.0  66600.0   9114352.0\n",
      "5   2023.08.28  66800.0   300.0  66800.0  67000.0  66500.0   5824628.0\n",
      "6          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "7          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "8          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "9   2023.08.25  67100.0  1100.0  67100.0  67400.0  66900.0   7032462.0\n",
      "10  2023.08.24  68200.0  1100.0  68300.0  68700.0  67900.0  15044463.0\n",
      "11  2023.08.23  67100.0   500.0  66700.0  67100.0  66400.0   9549352.0\n",
      "12  2023.08.22  66600.0     0.0  67200.0  67700.0  66300.0  10500242.0\n",
      "13  2023.08.21  66600.0   300.0  66600.0  67100.0  66300.0   9720067.0\n",
      "14         NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "            날짜       종가     전일비       시가       고가       저가         거래량\n",
      "0          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "1   2023.08.18  66300.0   400.0  66000.0  66700.0  65800.0  11745006.0\n",
      "2   2023.08.17  66700.0   300.0  66300.0  66800.0  66000.0  10778652.0\n",
      "3   2023.08.16  67000.0   300.0  66700.0  67100.0  66300.0  13174578.0\n",
      "4   2023.08.14  67300.0   200.0  67500.0  67900.0  66900.0   9352343.0\n",
      "5   2023.08.11  67500.0   500.0  68400.0  68800.0  67500.0   9781038.0\n",
      "6          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "7          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "8          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
      "9   2023.08.10  68000.0   900.0  68300.0  68500.0  67800.0  10227311.0\n",
      "10  2023.08.09  68900.0  1300.0  68000.0  69600.0  67900.0  17259673.0\n",
      "11  2023.08.08  67600.0   900.0  69000.0  69100.0  67400.0  14664709.0\n",
      "12  2023.08.07  68500.0   200.0  67700.0  69200.0  67600.0  10968505.0\n",
      "13  2023.08.04  68300.0   500.0  68800.0  69100.0  68200.0  12360193.0\n",
      "14         NaN      NaN     NaN      NaN      NaN      NaN         NaN\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "headers = { \n",
    "             'referer' : 'https://finance.naver.com/item/sise.naver?code=005930',\n",
    "             'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "            }\n",
    "\n",
    "N=12\n",
    "for pageNum in range(1, N+1):\n",
    "    url=f'https://finance.naver.com/item/sise_day.naver?code=005930&page={pageNum}' \n",
    "    req=requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "    result=pd.read_html(req.text, encoding='euc-kr')[0]\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d69f904-6f85-46ee-89b7-a937f09bfe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             날짜       종가     전일비       시가       고가       저가         거래량\n",
      "0    2024.01.31  72700.0  1600.0  73400.0  74000.0  72500.0  14874547.0\n",
      "1    2024.01.30  74300.0   100.0  75000.0  75300.0  73700.0  12244418.0\n",
      "2    2024.01.29  74400.0  1000.0  73800.0  75200.0  73500.0  13976521.0\n",
      "3    2024.01.26  73400.0   700.0  73700.0  74500.0  73300.0  11160062.0\n",
      "4    2024.01.25  74100.0   100.0  74200.0  74800.0  73700.0  11737747.0\n",
      "..          ...      ...     ...      ...      ...      ...         ...\n",
      "115  2023.08.10  68000.0   900.0  68300.0  68500.0  67800.0  10227311.0\n",
      "116  2023.08.09  68900.0  1300.0  68000.0  69600.0  67900.0  17259673.0\n",
      "117  2023.08.08  67600.0   900.0  69000.0  69100.0  67400.0  14664709.0\n",
      "118  2023.08.07  68500.0   200.0  67700.0  69200.0  67600.0  10968505.0\n",
      "119  2023.08.04  68300.0   500.0  68800.0  69100.0  68200.0  12360193.0\n",
      "\n",
      "[120 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "import time\n",
    "import random\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "def crawling(url, headers, soup):\n",
    "    # 종목에 따른 서로 다른 모든 페이지를 가지고 올때 사용되는 코드 (코드의 흐름을 파악해보기) 마지막 페이지를 자동으로 가져오는 코드!!\n",
    "    last_page = int(soup.select_one('td.pgRR').a['href'].split('=')[-1])\n",
    "    \n",
    "    df = None\n",
    "    count = 0\n",
    "    for page in range(1, last_page + 1):\n",
    "      req = requests.get(f'{url}&page={page}', headers=headers)\n",
    "      df = pd.concat([df, pd.read_html(req.text, encoding = \"euc-kr\")[0]], ignore_index=True)\n",
    "      if count > 10:\n",
    "        break\n",
    "      count += 1\n",
    "      time.sleep( random.uniform(2,4)) # ==> 우리가 계속해서 데이터를 들고오면 ip가 막힐수가있는데 time.sleep() 메서드를 써서 시간을 기다렸다가 데이터를 들고온다.\n",
    "\n",
    "    df.dropna(inplace=True) # ==> 결측치 None값을 다 없애준다.\n",
    "    df.reset_index(drop=True, inplace=True) # ==> 인덱스를 다시 정렬해준다.\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    company_code = '005930' # 삼성전자\n",
    "    url =\"https://finance.naver.com/item/sise_day.nhn?code=\" + company_code\n",
    "    \n",
    "    headers = { \n",
    "             'referer' : 'https://finance.naver.com/item/sise.naver?code=005930',\n",
    "             'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "            }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    result = crawling(url, headers, soup)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58faa4-a51b-4086-80c9-33576fcb9e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
